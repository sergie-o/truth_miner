# ğŸ“° TruthMiner: Fake News Detection with NLP & Transformers

<p align="center">
  <img src="https://github.com/sergie-o/truth_miner/blob/main/truthminer.png" width="900"/>
</p>

---

>## ğŸ‘€ What is TruthMiner?

>In todayâ€™s world, **misinformation travels faster than facts**.  
>TruthMiner is an AI-powered system that **detects fake news headlines** using both **classic Machine Learning** and **modern Transformer models (BERT)**.  

>Think of it as your **digital truth detector** â€” analyzing headlines and predicting whether theyâ€™re **real** or **fake**.  

---

## ğŸ“Œ Project Overview
**TruthMiner** is a Natural Language Processing (NLP) project designed to automatically classify news headlines as either **real** or **fake**.  
The project combines both **classical machine learning models** and a **fine-tuned Transformer (DistilBERT)** to benchmark performance and highlight the trade-offs between traditional approaches and modern deep learning.

---

## ğŸ¯ Goals
- Preprocess raw text into clean, usable features  
- Apply feature engineering with **TF-IDF** and embeddings  
- Train and compare multiple ML classifiers:  
  - Logistic Regression  
  - NaÃ¯ve Bayes  
  - Random Forest  
  - Support Vector Machine (SVM)  
  - XGBoost  
- Fine-tune **DistilBERT** for sequence classification  
- Evaluate models using **Accuracy, Precision, Recall, F1-score**  
- Generate predictions for the test dataset in the original file format  

---

## ğŸ“‚ Dataset
- **Training data:** `dataset/training_data.csv`  
  - Format: `label<TAB>headline`  
  - `label = 0` â†’ Fake News  
  - `label = 1` â†’ Real News  

- **Testing data:** `dataset/testing_data.csv`  
  - Same format, but labels are placeholders (`2`)  
  - Your model predicts and replaces them with `0` or `1`

---

## âš™ï¸ Workflow

### 1ï¸âƒ£ Preprocessing
- Convert to lowercase  
- Remove punctuation  
- Tokenize text  
- Remove stopwords  
- Apply stemming / lemmatization  

### 2ï¸âƒ£ Feature Engineering
- TF-IDF vectorization (unigrams + bigrams)  
- Embeddings for Transformer input  

### 3ï¸âƒ£ Modeling
- Train classical ML models with TF-IDF features  
- Fine-tune **DistilBERT** using Hugging Face `Trainer` API  
- Apply stratified splits (80/20) for validation  

### 4ï¸âƒ£ Evaluation
Metrics reported:
- **Accuracy**
- **Precision**
- **Recall**
- **F1-score**

### 5ï¸âƒ£ Prediction
- Replace `2` labels in the test set with predicted values  
- Save results as `testing_predictions.csv` (tab-separated, no header)  

---

## ğŸ“Š Results (Validation Set)

| Model                 | Accuracy | Precision | Recall | F1   |
|------------------------|----------|-----------|--------|------|
| Logistic Regression    | 0.94     | 0.94      | 0.94   | 0.94 |
| NaÃ¯ve Bayes            | 0.91     | 0.90      | 0.91   | 0.90 |
| Random Forest          | 0.89     | 0.88      | 0.89   | 0.88 |
| SVM (Linear)           | **0.95** | 0.94      | 0.95   | 0.95 |
| XGBoost                | 0.93     | 0.93      | 0.93   | 0.93 |
| DistilBERT (fine-tuned)| 0.96     | 0.96      | 0.96   | 0.96 |

> âš¡ Transformers slightly outperformed classical ML, but SVM and Logistic Regression were strong baselines.

---

## ğŸ› ï¸ Tech Stack
- **Languages:** Python 3.11  
- **Libraries:**  
  - `pandas`, `numpy`, `scikit-learn`  
  - `xgboost`  
  - `torch`, `transformers`, `datasets`  
- **Environment:** macOS M3 Air, VS Code  
- **Hardware:** CPU + Apple MPS (GPU acceleration)  

---

## ğŸš€ Usage

### Clone the repo
```bash
git clone https://github.com/yourusername/truthminer.git
cd truthminer
